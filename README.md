# ecommerce-data-pipeline
End-to-end data engineering project with ETL pipeline, data warehouse, and analytics

# ğŸ›ï¸ E-Commerce Analytics Data Pipeline

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![PostgreSQL](https://img.shields.io/badge/postgresql-15-blue.svg)
![Docker](https://img.shields.io/badge/docker-latest-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

> A production-ready end-to-end data engineering project demonstrating ETL pipelines, data warehousing, and analytics using modern data stack technologies.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Data Model](#data-model)
- [Analytics](#analytics)
- [Testing](#testing)
- [Screenshots](#screenshots)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This project implements a complete data engineering pipeline for an e-commerce analytics platform. It demonstrates:

- **ETL/ELT Processes**: Extracting data from CSV sources, transforming using SQL and Python
- **Data Warehousing**: Dimensional modeling with star schema (fact and dimension tables)
- **Data Quality**: Automated testing and validation
- **Analytics**: Interactive dashboards and business intelligence queries
- **Best Practices**: Version control, documentation, containerization, and CI/CD

**Perfect for**: Data Engineering portfolios, learning dimensional modeling, practicing SQL, and understanding modern data stacks.

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Data Files â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python ETL     â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  PostgreSQL  â”‚
â”‚  (src/loaders)  â”‚       â”‚   (Staging)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transform      â”‚
â”‚  (dimensions +  â”‚
â”‚   facts)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Warehouse â”‚
â”‚  (Star Schema)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SQL   â”‚   â”‚Streamlitâ”‚  â”‚ PgAdmin  â”‚
â”‚ Queries â”‚   â”‚Dashboardâ”‚  â”‚   UI     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## âœ¨ Features

- âœ… **Complete ETL Pipeline**: Automated data extraction, transformation, and loading
- âœ… **Star Schema Data Warehouse**: Optimized for analytics queries
- âœ… **Data Quality Tests**: Automated validation using pytest
- âœ… **Interactive Dashboard**: Real-time analytics with Streamlit
- âœ… **Docker Containerization**: Easy deployment and reproducibility
- âœ… **Sample Data Generator**: Faker-based realistic e-commerce data
- âœ… **10+ Business Analytics Queries**: Revenue, customers, products, trends
- âœ… **Professional Documentation**: Comprehensive README and code comments

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| **Language** | Python 3.9+ |
| **Database** | PostgreSQL 15 |
| **Containerization** | Docker, Docker Compose |
| **Data Processing** | Pandas, NumPy |
| **Database ORM** | SQLAlchemy |
| **Testing** | Pytest |
| **Dashboard** | Streamlit, Plotly |
| **Data Generation** | Faker |
| **Version Control** | Git, GitHub |

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Docker Desktop
- Git

### Installation
```bash
# 1. Clone the repository
git clone https://github.com/YOUR_USERNAME/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Start Docker containers
make start
# Or: docker-compose up -d

# 5. Generate sample data
make data
# Or: python -m src.utils.generate_sample_data

# 6. Run ETL pipeline
make pipeline
# Or: python -m src.run_pipeline

# 7. Launch dashboard
make dashboard
# Or: streamlit run dashboards/ecommerce_dashboard.py
```

Visit:
- **Dashboard**: http://localhost:8501
- **PgAdmin**: http://localhost:5050 (admin@admin.com / admin)

## ğŸ“ Project Structure

ecommerce-data-pipeline/
â”‚
â”œâ”€â”€ airflow/                 # Airflow DAGs (future enhancement)
â”œâ”€â”€ dashboards/              # Streamlit dashboards
â”‚   â””â”€â”€ ecommerce_dashboard.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw data files
â”‚   â”œâ”€â”€ processed/          # Processed data
â”‚   â””â”€â”€ sample/             # Generated sample data
â”œâ”€â”€ dbt/                    # DBT models (future enhancement)
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml  # Container orchestration
â”œâ”€â”€ docs/                   # Additional documentation
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ ddl/               # Data Definition Language scripts
â”‚   â”‚   â”œâ”€â”€ 01_create_schemas.sql
â”‚   â”‚   â”œâ”€â”€ 02_create_staging_tables.sql
â”‚   â”‚   â””â”€â”€ 03_create_marts_tables.sql
â”‚   â””â”€â”€ queries/           # Analytics queries
â”‚       â””â”€â”€ business_analytics.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/        # Data extraction modules
â”‚   â”œâ”€â”€ loaders/           # Data loading modules
â”‚   â”‚   â””â”€â”€ csv_to_postgres.py
â”‚   â”œâ”€â”€ transformers/      # Data transformation modules
â”‚   â”‚   â”œâ”€â”€ load_dimensions.py
â”‚   â”‚   â””â”€â”€ load_facts.py
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ db_connection.py
â”‚   â”‚   â”œâ”€â”€ generate_sample_data.py
â”‚   â”‚   â””â”€â”€ run_analytics.py
â”‚   â””â”€â”€ run_pipeline.py    # Main ETL orchestrator
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/              # Unit tests
â”‚   â””â”€â”€ integration/       # Integration tests
â”œâ”€â”€ .env                   # Environment variables (not in repo)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile              # Convenient commands
â”œâ”€â”€ pytest.ini            # Pytest configuration
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

## ğŸ“Š Data Model

### Star Schema Design

**Dimension Tables:**
- `dim_customers` - Customer information and segmentation
- `dim_products` - Product catalog with pricing
- `dim_date` - Date dimension for time-based analysis

**Fact Tables:**
- `fact_orders` - Order-level metrics (revenue, profit, items)
- `fact_order_items` - Line-item level details

### Entity Relationship Diagram

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_customers  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ customer_key PK â”‚
â”‚ customer_id     â”‚
â”‚ full_name       â”‚
â”‚ email           â”‚
â”‚ country         â”‚
â”‚ segment         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚ 1:N
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  fact_orders    â”‚  N:1â”‚   dim_date      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â—„â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ order_key    PK â”‚     â”‚ date_key     PK â”‚
â”‚ customer_key FK â”‚     â”‚ date            â”‚
â”‚ date_key     FK â”‚     â”‚ year, month     â”‚
â”‚ total_amount    â”‚     â”‚ quarter, week   â”‚
â”‚ profit          â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚ 1:N
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fact_order_items  â”‚ N:1â”‚  dim_products   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â—„â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ item_key      PK  â”‚    â”‚ product_key  PK â”‚
â”‚ order_key     FK  â”‚    â”‚ product_id      â”‚
â”‚ product_key   FK  â”‚    â”‚ product_name    â”‚
â”‚ quantity          â”‚    â”‚ category        â”‚
â”‚ total_price       â”‚    â”‚ price, cost     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## ğŸ“ˆ Analytics

The project includes 10+ pre-built analytics queries:

1. **Sales Overview** - KPIs for revenue, orders, customers
2. **Daily Sales Trend** - Time-series analysis
3. **Top Customers** - Customer lifetime value
4. **Product Performance** - Revenue by category/product
5. **Customer Segmentation** - Loyal vs New customers
6. **Monthly Trends** - Month-over-month growth
7. **Order Status Distribution** - Completion rates
8. **Weekend vs Weekday** - Day-of-week patterns
9. **Geographic Analysis** - Revenue by country
10. **Cohort Analysis** - Customer retention

## ğŸ§ª Testing
```bash
# Run all tests
make test

# Run with coverage
make test-coverage

# Unit tests only
make test-unit

# Integration tests only
make test-integration
```

**Test Coverage:**
- Unit tests for data generators and utilities
- Integration tests for database connections and data quality
- Data validation tests for schema and relationships
- End-to-end pipeline tests

## ğŸ“¸ Screenshots

### Interactive Dashboard
![Dashboard](docs/images/dashboard.png)

### Data Model in PgAdmin
![Database](docs/images/database_schema.png)

### ETL Pipeline Execution
![Pipeline](docs/images/pipeline_run.png)

## ğŸ”§ Available Commands
```bash
make help         # Show all available commands
make start        # Start Docker containers
make stop         # Stop Docker containers
make data         # Generate sample data
make pipeline     # Run complete ETL pipeline
make dashboard    # Launch Streamlit dashboard
make analytics    # Run analytics queries
make test         # Run all tests
make clean        # Clean data and restart
```

## ğŸ“ Skills Demonstrated

This project showcases the following data engineering skills:

### Technical Skills
- **Python Programming**: OOP, pandas, SQLAlchemy, async processing
- **SQL Mastery**: DDL, DML, CTEs, window functions, complex joins
- **Database Design**: Star schema, normalization, indexing
- **ETL/ELT**: Data extraction, transformation, loading patterns
- **Data Modeling**: Dimensional modeling, slowly changing dimensions
- **Testing**: Unit tests, integration tests, data quality validation
- **Version Control**: Git workflows, branching, documentation

### Tools & Technologies
- **Databases**: PostgreSQL, SQL optimization
- **Containerization**: Docker, Docker Compose
- **Orchestration**: Python scripts (extensible to Airflow)
- **Visualization**: Streamlit, Plotly
- **Data Generation**: Faker for realistic test data
- **CI/CD**: GitHub Actions (future enhancement)

### Best Practices
- âœ… Clean, modular code architecture
- âœ… Comprehensive documentation
- âœ… Automated testing
- âœ… Error handling and logging
- âœ… Configuration management
- âœ… Database migrations and version control
- âœ… Reproducible environments

## ğŸ”® Future Enhancements

### Phase 1 (Planned)
- [ ] Apache Airflow DAGs for orchestration
- [ ] DBT for transformations
- [ ] GitHub Actions CI/CD pipeline
- [ ] Data lineage tracking
- [ ] Monitoring and alerting

### Phase 2 (Roadmap)
- [ ] Real-time streaming with Apache Kafka
- [ ] Cloud deployment (AWS/GCP/Azure)
- [ ] Data catalog integration (DataHub/Amundsen)
- [ ] ML model serving pipeline
- [ ] Advanced analytics (cohort analysis, forecasting)
- [ ] API layer with FastAPI

### Phase 3 (Ideas)
- [ ] Multi-source ingestion (APIs, databases)
- [ ] Incremental loading strategies
- [ ] Data versioning and time travel
- [ ] Cost optimization dashboard
- [ ] Multi-tenant architecture

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [your-linkedin](https://linkedin.com/in/your-profile)
- Email: your.email@example.com

## ğŸ™ Acknowledgments

- Inspired by real-world data engineering challenges
- Built as a learning project and portfolio piece
- Thanks to the open-source community for amazing tools

## ğŸ“š Resources

- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Docker Documentation](https://docs.docker.com/)
- [Data Warehouse Toolkit by Kimball](https://www.kimballgroup.com/)

---

â­ **If you found this project helpful, please star the repository!**

Made with â¤ï¸ for the Data Engineering community